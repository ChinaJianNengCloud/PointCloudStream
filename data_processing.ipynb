{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('data/dara_pose_r_cam_u_lincheng'), PosixPath('data/data_pose_r_cam_u_lincheng2'), PosixPath('data/data_pose_r_cam_u_3'), PosixPath('data/data_pose_r_cam_u_1'), PosixPath('data/data_ss_2'), PosixPath('data/data_pose_r_cam_u_lincheng'), PosixPath('data/data_ss_'), PosixPath('data/data_pose_r_cam_u_lin'), PosixPath('data/data_pose_r_cam_u_2'), PosixPath('data/dara_pose_r_cam_u_lin'), PosixPath('data/resources'), PosixPath('data/data_pose_r_cam_u_4'), PosixPath('data/data_pose_r_cam_u_lincheng3')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"./data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "12\n",
      "15\n",
      "6\n",
      "1\n",
      "1\n",
      "81\n",
      "10\n",
      "10\n",
      "13\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "file_contents = {}\n",
    "for json_f in path.rglob(\"sa*.json\"):\n",
    "    json_contents = json.loads(json_f.read_text())\n",
    "    print(len(json_contents))\n",
    "    file_contents.update(json_contents)\n",
    "    # break\n",
    "\n",
    "# Sort the file_contents by the 'prompt' field\n",
    "sorted_contents = OrderedDict(\n",
    "    sorted(file_contents.items(), key=lambda item: int(item[1]['prompt']))\n",
    ")\n",
    "\n",
    "output = Path(\"./data\") / \"lincheng\"\n",
    "output.mkdir(exist_ok=True)\n",
    "with open(output / \"saved_data.json\", \"w\") as f:\n",
    "    json.dump(sorted_contents, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yolo finetun using different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from ultralytics.utils.checks import check_requirements\n",
    "from ultralytics.utils.downloads import download\n",
    "from ultralytics.utils.ops import xyxy2xywhn\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "check_requirements(('pycocotools>=2.0',))\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Make Directories\n",
    "dir = Path('/home/capre/disk_4/yutao/ultralytics/datasets/Objects365')  # dataset root dir\n",
    "for p in 'images', 'labels':\n",
    "    (dir / p).mkdir(parents=True, exist_ok=True)\n",
    "    for q in 'train', 'val':\n",
    "        (dir / p / q).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Train, Val Splits\n",
    "for split, patches in [('train', 50 + 1), ('val', 43 + 1)]:\n",
    "    print(f\"Processing {split} in {patches} patches ...\")\n",
    "    images, labels = dir / 'images' / split, dir / 'labels' / split\n",
    "\n",
    "    # Download\n",
    "    url = f\"https://dorc.ks3-cn-beijing.ksyun.com/data-set/2020Objects365%E6%95%B0%E6%8D%AE%E9%9B%86/{split}/\"\n",
    "    if split == 'train':\n",
    "        download([f'{url}zhiyuan_objv2_{split}.tar.gz'], dir=dir)  # annotations json\n",
    "        download([f'{url}patch{i}.tar.gz' for i in range(patches)], dir=images, curl=True, threads=8)\n",
    "    elif split == 'val':\n",
    "        download([f'{url}zhiyuan_objv2_{split}.json'], dir=dir)  # annotations json\n",
    "        download([f'{url}images/v1/patch{i}.tar.gz' for i in range(15 + 1)], dir=images, curl=True, threads=8)\n",
    "        download([f'{url}images/v2/patch{i}.tar.gz' for i in range(16, patches)], dir=images, curl=True, threads=8)\n",
    "\n",
    "    # Move\n",
    "    for f in tqdm(images.rglob('*.jpg'), desc=f'Moving {split} images'):\n",
    "        f.rename(images / f.name)  # move to /images/{split}\n",
    "\n",
    "    # Labels\n",
    "    coco = COCO(dir / f'zhiyuan_objv2_{split}.json')\n",
    "    names = [x[\"name\"] for x in coco.loadCats(coco.getCatIds())]\n",
    "    for cid, cat in enumerate(names):\n",
    "        catIds = coco.getCatIds(catNms=[cat])\n",
    "        imgIds = coco.getImgIds(catIds=catIds)\n",
    "        for im in tqdm(coco.loadImgs(imgIds), desc=f'Class {cid + 1}/{len(names)} {cat}'):\n",
    "            width, height = im[\"width\"], im[\"height\"]\n",
    "            path = Path(im[\"file_name\"])  # image filename\n",
    "            try:\n",
    "                with open(labels / path.with_suffix('.txt').name, 'a') as file:\n",
    "                    annIds = coco.getAnnIds(imgIds=im[\"id\"], catIds=catIds, iscrowd=None)\n",
    "                    for a in coco.loadAnns(annIds):\n",
    "                        x, y, w, h = a['bbox']  # bounding box in xywh (xy top-left corner)\n",
    "                        xyxy = np.array([x, y, x + w, y + h])[None]  # pixels(1,4)\n",
    "                        x, y, w, h = xyxy2xywhn(xyxy, w=width, h=height, clip=True)[0]  # normalized and clipped\n",
    "                        file.write(f\"{cid} {x:.5f} {y:.5f} {w:.5f} {h:.5f}\\n\")\n",
    "            except Exception as e:\n",
    "                print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "# os.chdir(\"/home/capre/disk_4/yutao/ultralytics\")\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11x-seg.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=\"Objects365.yaml\", epochs=100, imgsz=640)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yolo finetune for breast seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11x-seg.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=\"/home/capre/disk_4/yutao/breast-seg/dataset3/dataset.yaml\", epochs=100, imgsz=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# masks filename align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def add_leading_zeros_to_masks(masks_dir, total_digits=4):\n",
    "    \"\"\"\n",
    "    将 masks_dir 中的文件名加上前导零，使其成为指定长度的数字字符串。\n",
    "\n",
    "    参数：\n",
    "    - masks_dir: 掩码文件夹的路径\n",
    "    - total_digits: 文件名应达到的总位数，默认为4\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(masks_dir):\n",
    "        # 分离文件名和扩展名\n",
    "        basename, extension = os.path.splitext(filename)\n",
    "        try:\n",
    "            # 将文件名转换为整数，以确保文件名是数字\n",
    "            number = int(basename)\n",
    "            # 格式化新的文件名，添加前导零\n",
    "            new_basename = f\"{number:0{total_digits}d}\"\n",
    "            new_filename = new_basename + extension\n",
    "            # 构建完整的源和目标路径\n",
    "            src = os.path.join(masks_dir, filename)\n",
    "            dst = os.path.join(masks_dir, new_filename)\n",
    "            # 重命名文件\n",
    "            os.rename(src, dst)\n",
    "            print(f\"重命名：{filename} -> {new_filename}\")\n",
    "        except ValueError:\n",
    "            print(f\"跳过非数字文件名：{filename}\")\n",
    "\n",
    "# 使用示例：\n",
    "masks_dir = '/home/capre/disk_4/yutao/breast-seg/masks'  # 替换为您的 masks 文件夹路径\n",
    "add_leading_zeros_to_masks(masks_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yolo dataset make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "images_dir = '/home/capre/disk_4/yutao/breast-seg/images'  # Replace with your images directory\n",
    "masks_dir = '/home/capre/disk_4/yutao/breast-seg/masks'      # Replace with your masks directory\n",
    "dataset_root = '/home/capre/disk_4/yutao/breast-seg/dataset2'  # Replace with your dataset root directory\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(os.path.join(dataset_root, 'images', 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_root, 'images', 'val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_root, 'labels', 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_root, 'labels', 'val'), exist_ok=True)\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [f for f in os.listdir(images_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Shuffle and split into train and val\n",
    "random.shuffle(image_files)\n",
    "split_index = int(len(image_files) * 0.8)\n",
    "train_files = image_files[:split_index]\n",
    "val_files = image_files[split_index:]\n",
    "\n",
    "def process_dataset(phase, files):\n",
    "    for image_file in files:\n",
    "        # Read image and mask\n",
    "        image_path = os.path.join(images_dir, image_file)\n",
    "        mask_path = os.path.join(masks_dir, image_file)  # Assuming mask has the same name\n",
    "        image = cv2.imread(image_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Check if mask exists\n",
    "        if mask is None:\n",
    "            print(f\"Mask not found for image {image_file}\")\n",
    "            continue\n",
    "\n",
    "        height, width = mask.shape\n",
    "        # Threshold mask to binary\n",
    "        _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Create label file\n",
    "        label_file = os.path.splitext(image_file)[0] + '.txt'\n",
    "        label_path = os.path.join(dataset_root, 'labels', phase, label_file)\n",
    "\n",
    "        with open(label_path, 'w') as f:\n",
    "            for contour in contours:\n",
    "                # Simplify contour\n",
    "                epsilon = 0.001 * cv2.arcLength(contour, True)\n",
    "                contour = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "                # Flatten contour array\n",
    "                contour = contour.squeeze()\n",
    "                if contour.ndim != 2:\n",
    "                    continue  # Skip if contour is not 2D\n",
    "\n",
    "                # Normalize coordinates\n",
    "                normalized_contour = contour.astype(np.float32)\n",
    "                normalized_contour[:, 0] /= width\n",
    "                normalized_contour[:, 1] /= height\n",
    "\n",
    "                # Flatten and convert to list\n",
    "                contour_list = normalized_contour.flatten().tolist()\n",
    "\n",
    "                # Write to file (class index is 0)\n",
    "                line = '0 ' + ' '.join(map(str, contour_list))\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "        # Copy image to dataset folder\n",
    "        shutil.copy(image_path, os.path.join(dataset_root, 'images', phase, image_file))\n",
    "\n",
    "        # Visualization (optional)\n",
    "        # if random.random() < 0.05:  # Adjust the probability as needed\n",
    "        #     # Plot image and contours\n",
    "        #     plt.figure(figsize=(10, 10))\n",
    "        #     plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        #     for contour in contours:\n",
    "        #         contour = contour.squeeze()\n",
    "        #         plt.plot(contour[:, 0], contour[:, 1], linewidth=2)\n",
    "        #     plt.title(f\"{phase} - {image_file}\")\n",
    "        #     plt.axis('off')\n",
    "        #     plt.show()\n",
    "\n",
    "# Process train and val datasets\n",
    "process_dataset('train', train_files)\n",
    "process_dataset('val', val_files)\n",
    "\n",
    "# Generate dataset YAML file\n",
    "dataset_yaml = os.path.join(dataset_root, 'dataset.yaml')\n",
    "with open(dataset_yaml, 'w') as f:\n",
    "    f.write(f\"path: {dataset_root}\\n\")\n",
    "    f.write(\"train: images/train\\n\")\n",
    "    f.write(\"val: images/val\\n\")\n",
    "    f.write(\"test: \\n\\n\")\n",
    "    f.write(\"names:\\n\")\n",
    "    f.write(\"  0: object\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add label from predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Define paths\n",
    "images_dir = '/home/capre/disk_4/yutao/breast-seg/images'  # Replace with your images directory\n",
    "masks_dir = '/home/capre/disk_4/yutao/breast-seg/masks'      # Replace with your masks directory\n",
    "dataset_root = '/home/capre/disk_4/yutao/breast-seg/dataset3'  # Replace with your dataset root directory\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(os.path.join(dataset_root, 'images', 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_root, 'images', 'val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_root, 'labels', 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_root, 'labels', 'val'), exist_ok=True)\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [f for f in os.listdir(images_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Shuffle and split into train and val\n",
    "random.shuffle(image_files)\n",
    "split_index = int(len(image_files) * 0.8)\n",
    "train_files = image_files[:split_index]\n",
    "val_files = image_files[split_index:]\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO('yolo11x-seg.pt')  # Replace with the correct path to your model\n",
    "\n",
    "# Get class names and assign a new class ID for your mask label\n",
    "class_names = model.names  # A dictionary {class_id: class_name}\n",
    "max_class_id = max(class_names.keys())\n",
    "mask_class_id = max_class_id + 1\n",
    "class_names[mask_class_id] = 'breast'  # Replace 'breast' with your class name\n",
    "\n",
    "def process_dataset(phase, files):\n",
    "    for image_file in files:\n",
    "        # Read image and mask\n",
    "        image_path = os.path.join(images_dir, image_file)\n",
    "        mask_path = os.path.join(masks_dir, image_file)  # Assuming mask has the same name\n",
    "        image = cv2.imread(image_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Check if mask exists\n",
    "        if mask is None:\n",
    "            print(f\"Mask not found for image {image_file}\")\n",
    "            continue\n",
    "\n",
    "        original_height, original_width = image.shape[:2]\n",
    "\n",
    "        # Run the model to get predictions\n",
    "        results = model.predict(source=image, save=False, verbose=False)\n",
    "        result = results[0]\n",
    "\n",
    "        # Create label file\n",
    "        label_file = os.path.splitext(image_file)[0] + '.txt'\n",
    "        label_path = os.path.join(dataset_root, 'labels', phase, label_file)\n",
    "\n",
    "        with open(label_path, 'w') as f:\n",
    "            # Process predicted masks\n",
    "            if hasattr(result, 'masks') and result.masks is not None:\n",
    "                pred_masks = result.masks.data.cpu().numpy()\n",
    "                pred_classes = result.boxes.cls.cpu().numpy().astype(int)\n",
    "                for mask_pred, class_id in zip(pred_masks, pred_classes):\n",
    "                    # Convert mask to binary image\n",
    "                    mask_pred = (mask_pred > 0.5).astype(np.uint8) * 255\n",
    "\n",
    "                    # Resize mask back to original image size\n",
    "                    mask_pred_resized = cv2.resize(mask_pred, (original_width, original_height), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                    # Find contours\n",
    "                    contours, _ = cv2.findContours(mask_pred_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "                    for contour in contours:\n",
    "                        # Simplify contour\n",
    "                        epsilon = 0.001 * cv2.arcLength(contour, True)\n",
    "                        contour = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "                        # Flatten contour array\n",
    "                        contour = contour.squeeze()\n",
    "                        if contour.ndim != 2:\n",
    "                            continue  # Skip if contour is not 2D\n",
    "\n",
    "                        # Normalize coordinates\n",
    "                        normalized_contour = contour.astype(np.float32)\n",
    "                        normalized_contour[:, 0] /= original_width\n",
    "                        normalized_contour[:, 1] /= original_height\n",
    "\n",
    "                        # Flatten and convert to list\n",
    "                        contour_list = normalized_contour.flatten().tolist()\n",
    "\n",
    "                        # Write to file\n",
    "                        line = f\"{class_id} \" + ' '.join(map(str, contour_list))\n",
    "                        f.write(line + '\\n')\n",
    "            else:\n",
    "                print(f\"No predicted masks for image {image_file}\")\n",
    "\n",
    "            # Process your own mask\n",
    "            # Threshold mask to binary\n",
    "            _, mask_bin = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # Find contours\n",
    "            contours, _ = cv2.findContours(mask_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            for contour in contours:\n",
    "                # Simplify contour\n",
    "                epsilon = 0.001 * cv2.arcLength(contour, True)\n",
    "                contour = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "                # Flatten contour array\n",
    "                contour = contour.squeeze()\n",
    "                if contour.ndim != 2:\n",
    "                    continue  # Skip if contour is not 2D\n",
    "\n",
    "                # Normalize coordinates\n",
    "                normalized_contour = contour.astype(np.float32)\n",
    "                normalized_contour[:, 0] /= original_width\n",
    "                normalized_contour[:, 1] /= original_height\n",
    "\n",
    "                # Flatten and convert to list\n",
    "                contour_list = normalized_contour.flatten().tolist()\n",
    "\n",
    "                # Write to file with mask_class_id\n",
    "                line = f\"{mask_class_id} \" + ' '.join(map(str, contour_list))\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "        # Copy image to dataset folder\n",
    "        shutil.copy(image_path, os.path.join(dataset_root, 'images', phase, image_file))\n",
    "\n",
    "# Process train and val datasets\n",
    "process_dataset('train', train_files)\n",
    "process_dataset('val', val_files)\n",
    "\n",
    "# Generate dataset YAML file\n",
    "dataset_yaml = os.path.join(dataset_root, 'dataset.yaml')\n",
    "with open(dataset_yaml, 'w') as f:\n",
    "    f.write(f\"path: {dataset_root}\\n\")\n",
    "    f.write(\"train: images/train\\n\")\n",
    "    f.write(\"val: images/val\\n\")\n",
    "    f.write(\"test: \\n\\n\")\n",
    "    f.write(\"names:\\n\")\n",
    "    for class_id in sorted(class_names.keys()):\n",
    "        f.write(f\"  {class_id}: {class_names[class_id]}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "o3d311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
