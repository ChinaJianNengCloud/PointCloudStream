{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"./data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "file_contents = {}\n",
    "for json_f in path.rglob(\"sa*.json\"):\n",
    "    json_contents = json.loads(json_f.read_text())\n",
    "    print(len(json_contents))\n",
    "    file_contents.update(json_contents)\n",
    "    # break\n",
    "\n",
    "# Sort the file_contents by the 'prompt' field\n",
    "sorted_contents = OrderedDict(\n",
    "    sorted(file_contents.items(), key=lambda item: int(item[1]['prompt']))\n",
    ")\n",
    "\n",
    "output = Path(\"./data\") / \"lincheng\"\n",
    "output.mkdir(exist_ok=True)\n",
    "with open(output / \"saved_data.json\", \"w\") as f:\n",
    "    json.dump(sorted_contents, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yolo finetun using different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from ultralytics.utils.checks import check_requirements\n",
    "from ultralytics.utils.downloads import download\n",
    "from ultralytics.utils.ops import xyxy2xywhn\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "check_requirements(('pycocotools>=2.0',))\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Make Directories\n",
    "dir = Path('/home/capre/disk_4/yutao/ultralytics/datasets/Objects365')  # dataset root dir\n",
    "for p in 'images', 'labels':\n",
    "    (dir / p).mkdir(parents=True, exist_ok=True)\n",
    "    for q in 'train', 'val':\n",
    "        (dir / p / q).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Train, Val Splits\n",
    "for split, patches in [('train', 50 + 1), ('val', 43 + 1)]:\n",
    "    print(f\"Processing {split} in {patches} patches ...\")\n",
    "    images, labels = dir / 'images' / split, dir / 'labels' / split\n",
    "\n",
    "    # Download\n",
    "    url = f\"https://dorc.ks3-cn-beijing.ksyun.com/data-set/2020Objects365%E6%95%B0%E6%8D%AE%E9%9B%86/{split}/\"\n",
    "    if split == 'train':\n",
    "        download([f'{url}zhiyuan_objv2_{split}.tar.gz'], dir=dir)  # annotations json\n",
    "        download([f'{url}patch{i}.tar.gz' for i in range(patches)], dir=images, curl=True, threads=8)\n",
    "    elif split == 'val':\n",
    "        download([f'{url}zhiyuan_objv2_{split}.json'], dir=dir)  # annotations json\n",
    "        download([f'{url}images/v1/patch{i}.tar.gz' for i in range(15 + 1)], dir=images, curl=True, threads=8)\n",
    "        download([f'{url}images/v2/patch{i}.tar.gz' for i in range(16, patches)], dir=images, curl=True, threads=8)\n",
    "\n",
    "    # Move\n",
    "    for f in tqdm(images.rglob('*.jpg'), desc=f'Moving {split} images'):\n",
    "        f.rename(images / f.name)  # move to /images/{split}\n",
    "\n",
    "    # Labels\n",
    "    coco = COCO(dir / f'zhiyuan_objv2_{split}.json')\n",
    "    names = [x[\"name\"] for x in coco.loadCats(coco.getCatIds())]\n",
    "    for cid, cat in enumerate(names):\n",
    "        catIds = coco.getCatIds(catNms=[cat])\n",
    "        imgIds = coco.getImgIds(catIds=catIds)\n",
    "        for im in tqdm(coco.loadImgs(imgIds), desc=f'Class {cid + 1}/{len(names)} {cat}'):\n",
    "            width, height = im[\"width\"], im[\"height\"]\n",
    "            path = Path(im[\"file_name\"])  # image filename\n",
    "            try:\n",
    "                with open(labels / path.with_suffix('.txt').name, 'a') as file:\n",
    "                    annIds = coco.getAnnIds(imgIds=im[\"id\"], catIds=catIds, iscrowd=None)\n",
    "                    for a in coco.loadAnns(annIds):\n",
    "                        x, y, w, h = a['bbox']  # bounding box in xywh (xy top-left corner)\n",
    "                        xyxy = np.array([x, y, x + w, y + h])[None]  # pixels(1,4)\n",
    "                        x, y, w, h = xyxy2xywhn(xyxy, w=width, h=height, clip=True)[0]  # normalized and clipped\n",
    "                        file.write(f\"{cid} {x:.5f} {y:.5f} {w:.5f} {h:.5f}\\n\")\n",
    "            except Exception as e:\n",
    "                print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "# os.chdir(\"/home/capre/disk_4/yutao/ultralytics\")\n",
    "# Load a model\n",
    "model = YOLO(\"/home/capre/Point-Cloud-Stream/runs/segment/train6/weights/best.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model\n",
    "# results = model.train(data=\"Objects365.yaml\", epochs=100, imgsz=640)\n",
    "image_path = '/home/capre/disk_4/yutao/data/resources/0a7ee4d0cf344e15a81c68be0be1fb96_color_2.png'\n",
    "res= model.predict(source=image_path, save=False, verbose=False)\n",
    "res[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yolo finetune for breast seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11x-seg.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=\"/home/capre/disk_4/yutao/breast-seg/dataset3/dataset.yaml\", epochs=100, imgsz=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# masks filename align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def add_leading_zeros_to_masks(masks_dir, total_digits=4):\n",
    "    \"\"\"\n",
    "    将 masks_dir 中的文件名加上前导零，使其成为指定长度的数字字符串。\n",
    "\n",
    "    参数：\n",
    "    - masks_dir: 掩码文件夹的路径\n",
    "    - total_digits: 文件名应达到的总位数，默认为4\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(masks_dir):\n",
    "        # 分离文件名和扩展名\n",
    "        basename, extension = os.path.splitext(filename)\n",
    "        try:\n",
    "            # 将文件名转换为整数，以确保文件名是数字\n",
    "            number = int(basename)\n",
    "            # 格式化新的文件名，添加前导零\n",
    "            new_basename = f\"{number:0{total_digits}d}\"\n",
    "            new_filename = new_basename + extension\n",
    "            # 构建完整的源和目标路径\n",
    "            src = os.path.join(masks_dir, filename)\n",
    "            dst = os.path.join(masks_dir, new_filename)\n",
    "            # 重命名文件\n",
    "            os.rename(src, dst)\n",
    "            print(f\"重命名：{filename} -> {new_filename}\")\n",
    "        except ValueError:\n",
    "            print(f\"跳过非数字文件名：{filename}\")\n",
    "\n",
    "# 使用示例：\n",
    "masks_dir = '/home/capre/disk_4/yutao/breast-seg/masks'  # 替换为您的 masks 文件夹路径\n",
    "add_leading_zeros_to_masks(masks_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yolo dataset make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "images_dir = '/home/capre/disk_4/yutao/breast-seg/images'  # Replace with your images directory\n",
    "masks_dir = '/home/capre/disk_4/yutao/breast-seg/masks'      # Replace with your masks directory\n",
    "dataset_root = '/home/capre/disk_4/yutao/breast-seg/dataset2'  # Replace with your dataset root directory\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(os.path.join(dataset_root, 'images', 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_root, 'images', 'val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_root, 'labels', 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_root, 'labels', 'val'), exist_ok=True)\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [f for f in os.listdir(images_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Shuffle and split into train and val\n",
    "random.shuffle(image_files)\n",
    "split_index = int(len(image_files) * 0.8)\n",
    "train_files = image_files[:split_index]\n",
    "val_files = image_files[split_index:]\n",
    "\n",
    "def process_dataset(phase, files):\n",
    "    for image_file in files:\n",
    "        # Read image and mask\n",
    "        image_path = os.path.join(images_dir, image_file)\n",
    "        mask_path = os.path.join(masks_dir, image_file)  # Assuming mask has the same name\n",
    "        image = cv2.imread(image_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Check if mask exists\n",
    "        if mask is None:\n",
    "            print(f\"Mask not found for image {image_file}\")\n",
    "            continue\n",
    "\n",
    "        height, width = mask.shape\n",
    "        # Threshold mask to binary\n",
    "        _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Create label file\n",
    "        label_file = os.path.splitext(image_file)[0] + '.txt'\n",
    "        label_path = os.path.join(dataset_root, 'labels', phase, label_file)\n",
    "\n",
    "        with open(label_path, 'w') as f:\n",
    "            for contour in contours:\n",
    "                # Simplify contour\n",
    "                epsilon = 0.001 * cv2.arcLength(contour, True)\n",
    "                contour = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "                # Flatten contour array\n",
    "                contour = contour.squeeze()\n",
    "                if contour.ndim != 2:\n",
    "                    continue  # Skip if contour is not 2D\n",
    "\n",
    "                # Normalize coordinates\n",
    "                normalized_contour = contour.astype(np.float32)\n",
    "                normalized_contour[:, 0] /= width\n",
    "                normalized_contour[:, 1] /= height\n",
    "\n",
    "                # Flatten and convert to list\n",
    "                contour_list = normalized_contour.flatten().tolist()\n",
    "\n",
    "                # Write to file (class index is 0)\n",
    "                line = '0 ' + ' '.join(map(str, contour_list))\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "        # Copy image to dataset folder\n",
    "        shutil.copy(image_path, os.path.join(dataset_root, 'images', phase, image_file))\n",
    "\n",
    "        # Visualization (optional)\n",
    "        # if random.random() < 0.05:  # Adjust the probability as needed\n",
    "        #     # Plot image and contours\n",
    "        #     plt.figure(figsize=(10, 10))\n",
    "        #     plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        #     for contour in contours:\n",
    "        #         contour = contour.squeeze()\n",
    "        #         plt.plot(contour[:, 0], contour[:, 1], linewidth=2)\n",
    "        #     plt.title(f\"{phase} - {image_file}\")\n",
    "        #     plt.axis('off')\n",
    "        #     plt.show()\n",
    "\n",
    "# Process train and val datasets\n",
    "process_dataset('train', train_files)\n",
    "process_dataset('val', val_files)\n",
    "\n",
    "# Generate dataset YAML file\n",
    "dataset_yaml = os.path.join(dataset_root, 'dataset.yaml')\n",
    "with open(dataset_yaml, 'w') as f:\n",
    "    f.write(f\"path: {dataset_root}\\n\")\n",
    "    f.write(\"train: images/train\\n\")\n",
    "    f.write(\"val: images/val\\n\")\n",
    "    f.write(\"test: \\n\\n\")\n",
    "    f.write(\"names:\\n\")\n",
    "    f.write(\"  0: object\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add label from predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Define paths\n",
    "images_dir = '/home/capre/disk_4/yutao/breast-seg/images'  # Replace with your images directory\n",
    "masks_dir = '/home/capre/disk_4/yutao/breast-seg/masks'      # Replace with your masks directory\n",
    "dataset_root = '/home/capre/disk_4/yutao/breast-seg/dataset3'  # Replace with your dataset root directory\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(os.path.join(dataset_root, 'images', 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_root, 'images', 'val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_root, 'labels', 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_root, 'labels', 'val'), exist_ok=True)\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [f for f in os.listdir(images_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Shuffle and split into train and val\n",
    "random.shuffle(image_files)\n",
    "split_index = int(len(image_files) * 0.8)\n",
    "train_files = image_files[:split_index]\n",
    "val_files = image_files[split_index:]\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO('yolo11x-seg.pt')  # Replace with the correct path to your model\n",
    "\n",
    "# Get class names and assign a new class ID for your mask label\n",
    "class_names = model.names  # A dictionary {class_id: class_name}\n",
    "max_class_id = max(class_names.keys())\n",
    "mask_class_id = max_class_id + 1\n",
    "class_names[mask_class_id] = 'breast'  # Replace 'breast' with your class name\n",
    "\n",
    "def process_dataset(phase, files):\n",
    "    for image_file in files:\n",
    "        # Read image and mask\n",
    "        image_path = os.path.join(images_dir, image_file)\n",
    "        mask_path = os.path.join(masks_dir, image_file)  # Assuming mask has the same name\n",
    "        image = cv2.imread(image_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Check if mask exists\n",
    "        if mask is None:\n",
    "            print(f\"Mask not found for image {image_file}\")\n",
    "            continue\n",
    "\n",
    "        original_height, original_width = image.shape[:2]\n",
    "\n",
    "        # Run the model to get predictions\n",
    "        results = model.predict(source=image, save=False, verbose=False)\n",
    "        result = results[0]\n",
    "\n",
    "        # Create label file\n",
    "        label_file = os.path.splitext(image_file)[0] + '.txt'\n",
    "        label_path = os.path.join(dataset_root, 'labels', phase, label_file)\n",
    "\n",
    "        with open(label_path, 'w') as f:\n",
    "            # Process predicted masks\n",
    "            if hasattr(result, 'masks') and result.masks is not None:\n",
    "                pred_masks = result.masks.data.cpu().numpy()\n",
    "                pred_classes = result.boxes.cls.cpu().numpy().astype(int)\n",
    "                for mask_pred, class_id in zip(pred_masks, pred_classes):\n",
    "                    # Convert mask to binary image\n",
    "                    mask_pred = (mask_pred > 0.5).astype(np.uint8) * 255\n",
    "\n",
    "                    # Resize mask back to original image size\n",
    "                    mask_pred_resized = cv2.resize(mask_pred, (original_width, original_height), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                    # Find contours\n",
    "                    contours, _ = cv2.findContours(mask_pred_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "                    for contour in contours:\n",
    "                        # Simplify contour\n",
    "                        epsilon = 0.001 * cv2.arcLength(contour, True)\n",
    "                        contour = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "                        # Flatten contour array\n",
    "                        contour = contour.squeeze()\n",
    "                        if contour.ndim != 2:\n",
    "                            continue  # Skip if contour is not 2D\n",
    "\n",
    "                        # Normalize coordinates\n",
    "                        normalized_contour = contour.astype(np.float32)\n",
    "                        normalized_contour[:, 0] /= original_width\n",
    "                        normalized_contour[:, 1] /= original_height\n",
    "\n",
    "                        # Flatten and convert to list\n",
    "                        contour_list = normalized_contour.flatten().tolist()\n",
    "\n",
    "                        # Write to file\n",
    "                        line = f\"{class_id} \" + ' '.join(map(str, contour_list))\n",
    "                        f.write(line + '\\n')\n",
    "            else:\n",
    "                print(f\"No predicted masks for image {image_file}\")\n",
    "\n",
    "            # Process your own mask\n",
    "            # Threshold mask to binary\n",
    "            _, mask_bin = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # Find contours\n",
    "            contours, _ = cv2.findContours(mask_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            for contour in contours:\n",
    "                # Simplify contour\n",
    "                epsilon = 0.001 * cv2.arcLength(contour, True)\n",
    "                contour = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "                # Flatten contour array\n",
    "                contour = contour.squeeze()\n",
    "                if contour.ndim != 2:\n",
    "                    continue  # Skip if contour is not 2D\n",
    "\n",
    "                # Normalize coordinates\n",
    "                normalized_contour = contour.astype(np.float32)\n",
    "                normalized_contour[:, 0] /= original_width\n",
    "                normalized_contour[:, 1] /= original_height\n",
    "\n",
    "                # Flatten and convert to list\n",
    "                contour_list = normalized_contour.flatten().tolist()\n",
    "\n",
    "                # Write to file with mask_class_id\n",
    "                line = f\"{mask_class_id} \" + ' '.join(map(str, contour_list))\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "        # Copy image to dataset folder\n",
    "        shutil.copy(image_path, os.path.join(dataset_root, 'images', phase, image_file))\n",
    "\n",
    "# Process train and val datasets\n",
    "process_dataset('train', train_files)\n",
    "process_dataset('val', val_files)\n",
    "\n",
    "# Generate dataset YAML file\n",
    "dataset_yaml = os.path.join(dataset_root, 'dataset.yaml')\n",
    "with open(dataset_yaml, 'w') as f:\n",
    "    f.write(f\"path: {dataset_root}\\n\")\n",
    "    f.write(\"train: images/train\\n\")\n",
    "    f.write(\"val: images/val\\n\")\n",
    "    f.write(\"test: \\n\\n\")\n",
    "    f.write(\"names:\\n\")\n",
    "    for class_id in sorted(class_names.keys()):\n",
    "        f.write(f\"  {class_id}: {class_names[class_id]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEO Dataset Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Data Process (for xyz rxyz is invert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import pandas as pd\n",
    "data_path = '/home/capre/disk_4/yutao/leo_data/data_2nd'\n",
    "\n",
    "\n",
    "def switch_xyzrxyz(rxyzxyz:list):\n",
    "    r_xyz, xyz = rxyzxyz[0:3], rxyzxyz[3:6]\n",
    "    xyzrxyz = np.hstack((xyz, r_xyz))\n",
    "    return xyzrxyz.tolist()\n",
    "\n",
    "# print(switch_xyzrxyz([1,2,3,4,5,6]))\n",
    "\n",
    "def transform_pose(pose: list, T_cam_to_base):\n",
    "    pose: np.ndarray = np.array(pose)\n",
    "    t_xyz = pose[0:3]\n",
    "    r_xyz = pose[3:6]\n",
    "    rotation_matrix = R.from_euler('xyz', r_xyz.reshape(1, 3), degrees=False).as_matrix().reshape(3, 3)\n",
    "    T_end_to_base = np.eye(4)\n",
    "    T_end_to_base[:3, :3] = rotation_matrix\n",
    "    T_end_to_base[:3, 3] = t_xyz.ravel()\n",
    "    T_base_to_cam = np.linalg.inv(T_cam_to_base)\n",
    "\n",
    "    T_cam_to_end = T_base_to_cam @ T_end_to_base\n",
    "    new_t = T_cam_to_end[:3, 3]\n",
    "    new_r = R.from_matrix(T_cam_to_end[:3, :3]).as_euler('xyz', degrees=False)\n",
    "    xyzrxrzry = np.hstack((new_r, new_t.reshape(-1)))\n",
    "    return xyzrxrzry.tolist()\n",
    "\n",
    "\n",
    "path = Path(data_path)\n",
    "resources_path: Path = path / 'resources'\n",
    "all_data = {}\n",
    "x_values, y_values, z_values = [], [], []\n",
    "\n",
    "\n",
    "import re\n",
    "pattern = re.compile(r'(\\d+$)')\n",
    "\n",
    "\n",
    "for json_f in path.rglob(\"*saved_data.json\"):\n",
    "    # related_calib_json = json_f.parent / 'Calibration_results' / 'calibration_results.json'\n",
    "    # if related_calib_json.exists():\n",
    "        # calib = json.load(open(related_calib_json))\n",
    "    saved_data = json.load(open(json_f))\n",
    "    save = True\n",
    "    for each_record in saved_data:\n",
    "        prompt = saved_data[each_record]['prompt']\n",
    "        if pattern.search(prompt):\n",
    "            print(pattern.sub(\"\", prompt))\n",
    "            saved_data[each_record]['prompt'] = pattern.sub(\"\", prompt)\n",
    "            \n",
    "        for each_image in saved_data[each_record]['color_files']:\n",
    "            if not resources_path.joinpath(each_image).exists():\n",
    "                save = False\n",
    "                break\n",
    "        if not save:\n",
    "            break\n",
    "        \n",
    "        # for idx, pose in enumerate(saved_data[each_record]['poses']):\n",
    "        #     saved_data[each_record]['poses'][idx]['pose0'] = switch_xyzrxyz(saved_data[each_record]['poses'][idx]['pose0'])\n",
    "        #     if saved_data[each_record]['poses'][idx]['pose1'] is not None:\n",
    "        #         saved_data[each_record]['poses'][idx]['pose1'] = switch_xyzrxyz(saved_data[each_record]['poses'][idx]['pose1'])\n",
    "\n",
    "\n",
    "        for idx, pose in enumerate(saved_data[each_record]['pose']):\n",
    "            # saved_data[each_record]['pose'][idx] = switch_xyzrxyz(pose)\n",
    "            x_values.append(saved_data[each_record]['pose'][idx][0])\n",
    "            y_values.append(saved_data[each_record]['pose'][idx][1])\n",
    "            z_values.append(saved_data[each_record]['pose'][idx][2])\n",
    "        # print(saved_data[each_record])\n",
    "        all_data[each_record] = saved_data[each_record]\n",
    "        # break\n",
    "\n",
    "# Compute min and max for x, y, z\n",
    "min_x, max_x = min(x_values), max(x_values)\n",
    "min_y, max_y = min(y_values), max(y_values)\n",
    "min_z, max_z = min(z_values), max(z_values)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Min x: {min_x}, Max x: {max_x}\")\n",
    "print(f\"Min y: {min_y}, Max y: {max_y}\")\n",
    "print(f\"Min z: {min_z}, Max z: {max_z}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map id to real prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_map = pd.read_csv(path / 'task_list.csv')\n",
    "prompts_map['ID'] = prompts_map['ID'].astype(int).astype(str)\n",
    "\n",
    "for each_record in all_data:\n",
    "    all_data[each_record]['prompt'] = prompts_map[prompts_map['ID'] == all_data[each_record]['prompt']]['Prompts'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "all_data_len = len(all_data)\n",
    "random_keys = random.sample(list(all_data.keys()), all_data_len)\n",
    "all_data = {key: all_data[key] for key in random_keys}\n",
    "print(len(all_data))\n",
    "json.dump(all_data, open(path / 'all_data.json', 'w'), indent=4)\n",
    "\n",
    "half_data_len = len(all_data) // 2\n",
    "random_keys = random.sample(list(all_data.keys()), half_data_len)\n",
    "half_data = {key: all_data[key] for key in random_keys}\n",
    "print(len(half_data))\n",
    "json.dump(half_data, open(path / 'half_data.json', 'w'), indent=4)\n",
    "\n",
    "half_half_data_len = len(all_data) // 2 // 2\n",
    "random_keys = random.sample(list(all_data.keys()), half_half_data_len)\n",
    "half_half_data = {key: all_data[key] for key in random_keys}\n",
    "print(len(half_half_data))\n",
    "json.dump(half_half_data, open(path / 'half_half_data.json', 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cv2.goodFeaturesToTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plyfile\n",
    "data = plyfile.PlyData.read('/home/capre/disk_4/yutao/data/resources/0a7ee4d0cf344e15a81c68be0be1fb96_point_cloud_1.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python launch.py task=tuning_vla note=tuning_vla pretrained_ckpt_path=/home/capre/disk_4/yutao/leo/ckpts clip_txt_guidance.flag=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recalculate the segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "from plyfile import PlyElement, PlyData\n",
    "import open3d as o3d\n",
    "import open3d.core as o3c\n",
    "# os.chdir(\"/home/capre/disk_4/yutao/ultralytics\")\n",
    "# Load a model\n",
    "model = YOLO(\"/home/capre/Point-Cloud-Stream/runs/segment/train6/weights/best.pt\")  # load a pretrained model (recommended for training)\n",
    "o3d_device = o3d.core.Device(\"CUDA:0\")\n",
    "from utils.segmentation import segment_pcd_from_2d\n",
    "intrinsic = np.array([\n",
    "    [\n",
    "      610.5961520662408,\n",
    "      0.0,\n",
    "      639.8919938587554\n",
    "    ],\n",
    "    [\n",
    "      0.0,\n",
    "      617.4130735412369,\n",
    "      358.3889735843055\n",
    "    ],\n",
    "    [\n",
    "      0.0,\n",
    "      0.0,\n",
    "      1.0\n",
    "    ]\n",
    "  ]).T\n",
    "\n",
    "for idxx, each_record in enumerate(all_data):\n",
    "    print(f\"Processing {idxx}/{len(all_data)}: {each_record}\")\n",
    "    for idx, each_images in enumerate(all_data[each_record]['color_files']):\n",
    "        # print(each_image)\n",
    "        depth = resources_path / all_data[each_record]['depth_files'][idx]\n",
    "        color = resources_path / all_data[each_record]['color_files'][idx]\n",
    "        depth = np.load(str(depth))\n",
    "        depth = o3d.geometry.Image(depth)\n",
    "        color = o3d.io.read_image(str(color))\n",
    "        depth = o3d.t.geometry.Image.from_legacy(depth)\n",
    "        color = o3d.t.geometry.Image.from_legacy(color)\n",
    "        rgbd_image = o3d.t.geometry.RGBDImage(color, depth)\n",
    "        pcd_frame = o3d.t.geometry.PointCloud.create_from_rgbd_image(\n",
    "                    rgbd_image, \n",
    "                    o3c.Tensor(intrinsic, dtype=o3c.Dtype.Float32, device=o3d_device),\n",
    "                    o3c.Tensor(np.eye(4), dtype=o3c.Dtype.Float32, device=o3d_device),\n",
    "                    1000, 3,\n",
    "                    2, False)\n",
    "        xyz = np.asarray(pcd_frame.to_legacy().points)\n",
    "        rgb = np.asarray(pcd_frame.to_legacy().colors)\n",
    "        color_numpy = np.asarray(color.to_legacy())\n",
    "        print(color_numpy.shape)\n",
    "        # try:\n",
    "        label = segment_pcd_from_2d(model, pcd_frame, \n",
    "                                    resources_path / all_data[each_record]['color_files'][idx] \n",
    "                                    , intrinsic)\n",
    "        # except Exception as e:\n",
    "        #    print(resources_path / all_data[each_record]['color_files'][idx])\n",
    "        #    print(e)\n",
    "        pcd_with_labels = np.hstack((xyz, rgb, label.reshape(-1, 1)))\n",
    "        ply_name = resources_path / all_data[each_record]['point_cloud_files'][idx]\n",
    "        vertex = np.array(\n",
    "                    [(x, y, z, r, g, b, s) for x, y, z, r, g, b, s in pcd_with_labels],\n",
    "                    dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4'), ('red', 'u1'), ('green', 'u1'), ('blue', 'u1'),\n",
    "                           ('segment_id', 'i4')])\n",
    "        ply = PlyData([PlyElement.describe(vertex, 'vertex')], text=True)\n",
    "        ply.write(str(ply_name))\n",
    "\n",
    "    # break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"/home/capre/Point-Cloud-Stream/runs/segment/train6/weights/best.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# results = model.train(data=\"Objects365.yaml\", epochs=100, imgsz=640)\n",
    "image_path =  '/home/capre/disk_4/yutao/data/resources/200a4ee908534e36a0aae0e39e2c8854_color_3.png'\n",
    "res= model.predict(source=image_path, save=False, verbose=False)\n",
    "res[0].show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from plyfile import PlyElement, PlyData\n",
    "import open3d as o3d\n",
    "import open3d.core as o3c\n",
    "path = Path(r\"/home/capre/disk_4/yutao/leo_data/data_2nd\")\n",
    "resources_path = path / 'resources'\n",
    "# os.chdir(\"/home/capre/disk_4/yutao/ultralytics\")\n",
    "# Load a model\n",
    "model = YOLO(\"/home/capre/Point-Cloud-Stream/runs/segment/train6/weights/best.pt\")\n",
    "o3d_device = o3d.core.Device(\"CUDA:0\")\n",
    "from app.utils.camera.segmentation_utils import segment_pcd_from_2d\n",
    "intrinsic = np.array([\n",
    "    [\n",
    "      610.5961520662408,\n",
    "      0.0,\n",
    "      639.8919938587554\n",
    "    ],\n",
    "    [\n",
    "      0.0,\n",
    "      617.4130735412369,\n",
    "      358.3889735843055\n",
    "    ],\n",
    "    [\n",
    "      0.0,\n",
    "      0.0,\n",
    "      1.0\n",
    "    ]\n",
    "  ]).T\n",
    "\n",
    "for idxx, each_record in enumerate(all_data):\n",
    "    print(f\"Processing {idxx}/{len(all_data)}: {each_record}\")\n",
    "    \n",
    "    for idx, each_image in enumerate(all_data[each_record]['color_files']):\n",
    "        depth_path = resources_path / all_data[each_record]['depth_files'][idx]\n",
    "        color_path = resources_path / all_data[each_record]['color_files'][idx]\n",
    "\n",
    "        # Load depth and color images\n",
    "        depth = np.load(str(depth_path))\n",
    "        color = np.asarray(o3d.io.read_image(str(color_path)))\n",
    "\n",
    "        # Process 3-channel depth\n",
    "        if depth.ndim == 3 and depth.shape[-1] == 3:\n",
    "            # Decode depth if it's packed\n",
    "            depth = depth[:, :, 0] + depth[:, :, 1] * 256 + depth[:, :, 2] * 65536\n",
    "\n",
    "        # Generate point cloud\n",
    "        height, width = depth.shape\n",
    "        fx, fy = intrinsic[0, 0], intrinsic[1, 1]\n",
    "        cx, cy = intrinsic[0, 2], intrinsic[1, 2]\n",
    "\n",
    "        # Generate pixel grid\n",
    "        x, y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "        x = (x - cx) * depth / fx\n",
    "        y = (y - cy) * depth / fy\n",
    "        z = depth\n",
    "\n",
    "        # Stack XYZ coordinates\n",
    "        valid_mask = z > 0\n",
    "        xyz = np.stack((x, y, z), axis=-1)[valid_mask]\n",
    "        rgb = color[valid_mask]\n",
    "\n",
    "        # Segment point cloud\n",
    "        label = segment_pcd_from_2d(model, xyz, color_path, intrinsic)\n",
    "\n",
    "        # Combine XYZ, RGB, and labels\n",
    "        pcd_with_labels = np.hstack((xyz, rgb / 255.0, label.reshape(-1, 1)))\n",
    "\n",
    "        # Save as PLY\n",
    "        ply_name = resources_path / all_data[each_record]['point_cloud_files'][idx]\n",
    "        vertex = np.array(\n",
    "            [(x, y, z, r, g, b, s) for x, y, z, r, g, b, s in pcd_with_labels],\n",
    "            dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4'), ('red', 'f4'), ('green', 'f4'), ('blue', 'f4'), ('segment_id', 'i4')]\n",
    "        )\n",
    "        ply = PlyData([PlyElement.describe(vertex, 'vertex')], text=True)\n",
    "        ply.write(str(ply_name))\n",
    "\n",
    "    # Uncomment to debug with a single record\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch process seg pcd from yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from plyfile import PlyElement, PlyData\n",
    "import open3d as o3d\n",
    "import open3d.core as o3c\n",
    "import torch\n",
    "import cv2\n",
    "import torch.utils.dlpack\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize YOLO model\n",
    "model = YOLO(\"/home/capre/Point-Cloud-Stream/runs/segment/train6/weights/best.pt\")\n",
    "\n",
    "o3d_device = o3d.core.Device(\"CUDA:0\")\n",
    "\n",
    "# Intrinsic matrix\n",
    "intrinsic = np.array([\n",
    "    [610.5961520662408, 0.0,               639.8919938587554],\n",
    "    [0.0,               617.4130735412369, 358.3889735843055],\n",
    "    [0.0,               0.0,               1.0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "\n",
    "def o3d_t_to_torch(o3d_t):\n",
    "    return torch.utils.dlpack.from_dlpack(o3d_t.to_dlpack())\n",
    "\n",
    "def batch_segment_pcd_from_2d(\n",
    "    model: YOLO,\n",
    "    pcds: List[np.ndarray],\n",
    "    rgbs: List[np.ndarray],\n",
    "    full_images: List[np.ndarray],\n",
    "    intrinsics: List[np.ndarray],\n",
    "    extrinsics: List[np.ndarray],\n",
    "    threshold: float = 0.5,\n",
    "    device: str = 'cuda',\n",
    "    batch_size: int = 128\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform segmentation for a large set of point clouds and associated color images in batches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : YOLO\n",
    "        A YOLO segmentation model.\n",
    "    pcds : list of numpy arrays\n",
    "        Each is an Nx3 point cloud array (masked by valid depth).\n",
    "    rgbs : list of numpy arrays\n",
    "        Each is an Nx3 RGB array corresponding to pcds (masked by valid depth).\n",
    "    full_images : list of numpy arrays\n",
    "        Each is a full color image (HxWx3, uint8) for YOLO segmentation input.\n",
    "    intrinsics : list of numpy arrays\n",
    "        Each is a 3x3 intrinsic matrix.\n",
    "    extrinsics : list of numpy arrays\n",
    "        Each is a 4x4 extrinsic matrix.\n",
    "    threshold : float\n",
    "        Threshold for mask scores.\n",
    "    device : str\n",
    "        'cpu' or 'cuda'.\n",
    "    batch_size : int\n",
    "        Number of images to process per batch for YOLO inference.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels_list : list of numpy arrays\n",
    "        List of label arrays corresponding to each point cloud, shape (N,).\n",
    "    \"\"\"\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    total = len(pcds)\n",
    "    labels_list = [None] * total\n",
    "\n",
    "    with tqdm(total=total, desc=\"Processing Point Clouds\") as pbar:\n",
    "        for start_idx in range(0, total, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, total)\n",
    "            batch_pcds_sub = pcds[start_idx:end_idx]\n",
    "            batch_rgbs_sub = rgbs[start_idx:end_idx]\n",
    "            batch_full_images_sub = full_images[start_idx:end_idx]\n",
    "            batch_intrinsics_sub = intrinsics[start_idx:end_idx]\n",
    "            batch_extrinsics_sub = extrinsics[start_idx:end_idx]\n",
    "\n",
    "            # YOLO inference on the batch of full images\n",
    "            results = model.predict(source=batch_full_images_sub, device=device, verbose=False)\n",
    "\n",
    "            for i, (pcd, rgb, full_color_image, intrinsic_mat, extrinsic_mat) in enumerate(\n",
    "                zip(batch_pcds_sub, batch_rgbs_sub, batch_full_images_sub, batch_intrinsics_sub, batch_extrinsics_sub)\n",
    "            ):\n",
    "                idx = start_idx + i\n",
    "                res = results[i]\n",
    "\n",
    "                pcd_torch = torch.from_numpy(pcd).to(device, dtype=torch.float32)\n",
    "                intrinsic_torch = torch.from_numpy(intrinsic_mat).to(device, dtype=torch.float32)\n",
    "                extrinsic_torch = torch.from_numpy(extrinsic_mat).to(device, dtype=torch.float32)\n",
    "\n",
    "                # If no detections:\n",
    "                if res.masks is None or len(res.masks) == 0:\n",
    "                    labels_list[idx] = np.zeros((pcd.shape[0],), dtype=np.int64)\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                masks = res.masks.data.to(device)  # (num_masks, H', W')\n",
    "                labels_per_mask = res.boxes.cls.to(device)  # (num_masks,)\n",
    "\n",
    "                H, W = full_color_image.shape[:2]\n",
    "                # Resize masks to original size\n",
    "                masks_resized = torch.nn.functional.interpolate(\n",
    "                    masks.unsqueeze(1).float(),\n",
    "                    size=(H, W),\n",
    "                    mode='bilinear',\n",
    "                    align_corners=False\n",
    "                ).squeeze(1)  # (num_masks, H, W)\n",
    "\n",
    "                N = pcd_torch.shape[0]\n",
    "                ones = torch.ones((N, 1), device=device, dtype=torch.float32)\n",
    "                points_3d_hom = torch.cat([pcd_torch, ones], dim=1)  # (N,4)\n",
    "\n",
    "                # Transform to camera coordinates\n",
    "                points_cam_hom = (extrinsic_torch @ points_3d_hom.T).T  # (N,4)\n",
    "                points_cam = points_cam_hom[:, :3]\n",
    "\n",
    "                valid_depth = points_cam[:, 2] > 0\n",
    "                points_cam = points_cam[valid_depth]\n",
    "                indices = torch.nonzero(valid_depth).squeeze(1)\n",
    "\n",
    "                fx = intrinsic_torch[0, 0]\n",
    "                fy = intrinsic_torch[1, 1]\n",
    "                cx = intrinsic_torch[0, 2]\n",
    "                cy = intrinsic_torch[1, 2]\n",
    "\n",
    "                x_cam = points_cam[:, 0]\n",
    "                y_cam = points_cam[:, 1]\n",
    "                z_cam = points_cam[:, 2]\n",
    "                u = torch.round((x_cam * fx / z_cam) + cx).long()\n",
    "                v = torch.round((y_cam * fy / z_cam) + cy).long()\n",
    "\n",
    "                in_bounds = (u >= 0) & (u < W) & (v >= 0) & (v < H)\n",
    "                u = u[in_bounds]\n",
    "                v = v[in_bounds]\n",
    "                valid_indices = indices[in_bounds]\n",
    "\n",
    "                if u.numel() == 0:\n",
    "                    labels = torch.full((N,), 0, dtype=torch.int32, device=device)\n",
    "                    labels_list[idx] = labels.cpu().numpy()\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                mask_values = masks_resized[:, v, u]  # (num_masks, M)\n",
    "                mask_scores, mask_ids = torch.max(mask_values, dim=0)\n",
    "\n",
    "                valid_mask_points = mask_scores > threshold\n",
    "                final_indices = valid_indices[valid_mask_points]\n",
    "\n",
    "                point_labels = torch.full((N,), -1, dtype=torch.int32, device=device)\n",
    "                point_labels[final_indices] = labels_per_mask[mask_ids[valid_mask_points]].int()\n",
    "\n",
    "                labels = point_labels.cpu().numpy()\n",
    "                labels_list[idx] = labels\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    return labels_list\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Example Data Loading and Processing\n",
    "# --------------------------------------------------\n",
    "# The 'all_data' structure should be something like:\n",
    "# all_data = {\n",
    "#     record_key: {\n",
    "#         'color_files': [...], # list of filenames\n",
    "#         'depth_files': [...], # list of filenames\n",
    "#         'point_cloud_files': [...] # corresponding ply filenames\n",
    "#     },\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "path = Path(\"/home/capre/disk_4/yutao/leo_data/data_2nd\")\n",
    "resources_path = path / 'resources'\n",
    "\n",
    "batch_pcds = []\n",
    "batch_rgbs = []\n",
    "batch_full_colors = []\n",
    "batch_intrinsics = []\n",
    "batch_extrinsics = []\n",
    "batch_info = []  # (each_record, idx)\n",
    "\n",
    "record_keys = list(all_data.keys())\n",
    "for each_record in record_keys:\n",
    "    for idx, each_image in enumerate(all_data[each_record]['color_files']):\n",
    "        depth_path = resources_path / all_data[each_record]['depth_files'][idx]\n",
    "        color_path = resources_path / all_data[each_record]['color_files'][idx]\n",
    "\n",
    "        # Load depth\n",
    "        depth = np.load(str(depth_path))\n",
    "        # Load full color image\n",
    "        full_color = np.asarray(o3d.io.read_image(str(color_path)))  # HxWx3, uint8\n",
    "\n",
    "        # Process depth if it's 3-channel\n",
    "        if depth.ndim == 3 and depth.shape[-1] == 3:\n",
    "            depth = depth[:, :, 0] + depth[:, :, 1] * 256 + depth[:, :, 2] * 65536\n",
    "\n",
    "        height, width = depth.shape\n",
    "        fx, fy = intrinsic[0, 0], intrinsic[1, 1]\n",
    "        cx, cy = intrinsic[0, 2], intrinsic[1, 2]\n",
    "\n",
    "        x, y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "        X = (x - cx) * depth / fx\n",
    "        Y = (y - cy) * depth / fy\n",
    "        Z = depth\n",
    "\n",
    "        valid_mask = Z > 0\n",
    "        xyz = np.stack((X, Y, Z), axis=-1)[valid_mask]\n",
    "        rgb = full_color[valid_mask]\n",
    "\n",
    "        # Store masked xyz and rgb, and full image\n",
    "        batch_pcds.append(xyz)\n",
    "        batch_rgbs.append(rgb)\n",
    "        batch_full_colors.append(full_color)  # unmasked full image for YOLO\n",
    "        batch_intrinsics.append(intrinsic)\n",
    "        batch_extrinsics.append(np.eye(4))\n",
    "        batch_info.append((each_record, idx))\n",
    "\n",
    "# Run the segmentation in batches of 128\n",
    "labels_list = batch_segment_pcd_from_2d(\n",
    "    model=model,\n",
    "    pcds=batch_pcds,\n",
    "    rgbs=batch_rgbs,\n",
    "    full_images=batch_full_colors,\n",
    "    intrinsics=batch_intrinsics,\n",
    "    extrinsics=batch_extrinsics,\n",
    "    device='cuda',\n",
    "    batch_size=100\n",
    ")\n",
    "\n",
    "# Save results\n",
    "for i, labels in tqdm(enumerate(labels_list), desc='Saving ply', total=len(labels_list)):\n",
    "    xyz = batch_pcds[i]\n",
    "    rgb = batch_rgbs[i]\n",
    "    pcd_with_labels = np.hstack((xyz, rgb / 255.0, labels.reshape(-1, 1)))\n",
    "\n",
    "    each_record, idx = batch_info[i]\n",
    "    ply_name = resources_path / all_data[each_record]['point_cloud_files'][idx]\n",
    "\n",
    "    vertex = np.array(\n",
    "        [(x_p, y_p, z_p, r_p, g_p, b_p, s_p) for x_p, y_p, z_p, r_p, g_p, b_p, s_p in pcd_with_labels],\n",
    "        dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "               ('red', 'f4'), ('green', 'f4'), ('blue', 'f4'), ('segment_id', 'i4')]\n",
    "    )\n",
    "    ply = PlyData([PlyElement.describe(vertex, 'vertex')], text=True)\n",
    "    ply.write(str(ply_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from plyfile import PlyData, PlyElement\n",
    "import numpy as np\n",
    "\n",
    "def save_ply_file(i, labels, batch_pcds, batch_rgbs, batch_info, resources_path, all_data):\n",
    "    \"\"\"\n",
    "    Save a PLY file for the given index and associated data.\n",
    "\n",
    "    Parameters:\n",
    "    - i: Index of the data.\n",
    "    - labels: Labels for the point cloud.\n",
    "    - batch_pcds: List of point clouds (Nx3 arrays).\n",
    "    - batch_rgbs: List of RGB values for the points (Nx3 arrays).\n",
    "    - batch_info: List of (record, index) pairs for metadata.\n",
    "    - resources_path: Path to save the PLY files.\n",
    "    - all_data: Dictionary containing file paths for saving.\n",
    "    \"\"\"\n",
    "    xyz = batch_pcds[i]\n",
    "    rgb = batch_rgbs[i]\n",
    "    pcd_with_labels = np.hstack((xyz, rgb / 255.0, labels.reshape(-1, 1)))\n",
    "\n",
    "    each_record, idx = batch_info[i]\n",
    "    ply_name = resources_path / all_data[each_record]['point_cloud_files'][idx]\n",
    "\n",
    "    vertex = np.array(\n",
    "        [(x_p, y_p, z_p, r_p, g_p, b_p, s_p) for x_p, y_p, z_p, r_p, g_p, b_p, s_p in pcd_with_labels],\n",
    "        dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "               ('red', 'f4'), ('green', 'f4'), ('blue', 'f4'), ('segment_id', 'i4')]\n",
    "    )\n",
    "    ply = PlyData([PlyElement.describe(vertex, 'vertex')], text=True)\n",
    "    ply.write(str(ply_name))\n",
    "    del pcd_with_labels, ply\n",
    "\n",
    "\n",
    "def save_ply_files_parallel(labels_list, batch_pcds, batch_rgbs, batch_info, resources_path, all_data):\n",
    "    \"\"\"\n",
    "    Save PLY files using multiprocessing.\n",
    "\n",
    "    Parameters:\n",
    "    - labels_list: List of labels (one per point cloud).\n",
    "    - batch_pcds: List of point clouds (Nx3 arrays).\n",
    "    - batch_rgbs: List of RGB values for the points (Nx3 arrays).\n",
    "    - batch_info: List of (record, index) pairs for metadata.\n",
    "    - resources_path: Path to save the PLY files.\n",
    "    - all_data: Dictionary containing file paths for saving.\n",
    "    \"\"\"\n",
    "    # Prepare partial function for multiprocessing\n",
    "    save_ply_partial = partial(\n",
    "        save_ply_file,\n",
    "        batch_pcds=batch_pcds,\n",
    "        batch_rgbs=batch_rgbs,\n",
    "        batch_info=batch_info,\n",
    "        resources_path=resources_path,\n",
    "        all_data=all_data\n",
    "    )\n",
    "\n",
    "    # Use multiprocessing to save PLY files\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        list(tqdm(pool.imap(save_ply_partial, enumerate(labels_list)), desc=\"Saving PLY files\", total=len(labels_list)))\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# Example Usage\n",
    "# ---------------------------------\n",
    "# Assuming you already have:\n",
    "# - labels_list\n",
    "# - batch_pcds\n",
    "# - batch_rgbs\n",
    "# - batch_info\n",
    "# - resources_path\n",
    "# - all_data\n",
    "\n",
    "save_ply_files_parallel(labels_list, batch_pcds, batch_rgbs, batch_info, resources_path, all_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "o3d311vtk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
